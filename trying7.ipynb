{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trying7.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#!pip install underthesea\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "colab": {}
      },
      "source": [
        "!pip install underthesea\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import underthesea \n",
        "from underthesea import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stopword_set = stopwords.words('english')\n",
        "from keras.initializers import Constant\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dirr='/content/gdrive/My Drive/ColabNotebooks/my_text_generation/models/trying7'\n",
        "os.chdir(dirr)\n",
        "path_to_file = '/content/gdrive/My Drive/ColabNotebooks/corpus/paralle dataset/dataset.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = w.lower().strip()\n",
        "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r\"([\\'])\", r\" \\1\", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\" ?\\([^)]+\\)\", \"\", w)\n",
        "    return w\n",
        "def vie_preprocess(w):\n",
        "    w=word_tokenize(w,format='text')\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "def remove_eng_stopword(w,stopword_set):\n",
        "    w=nltk.tokenize.word_tokenize(w)\n",
        "    w=[w[i] for i in range(len(w)) if w[i] not in stopword_set]\n",
        "    w=' '.join(w)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    obs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    obs = [[re.sub(r' ','',word),remove_eng_stopword(gloss.split(';')[0],stopword_set), vie_preprocess(vie)] for word,gloss,vie in obs]\n",
        "    return zip(*obs)\n",
        "def tokenize(lang,vocab_size):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size,filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  lang_tokenizer.word_index=dict((word,idx) for word,idx in lang_tokenizer.word_index.items() if idx<=vocab_size)\n",
        "  lang_tokenizer.index_word=dict((idx,word) for idx,word in lang_tokenizer.index_word.items() if idx<=vocab_size)\n",
        "  return lang_tokenizer\n",
        "def padding(text,tokenizer):\n",
        "    seqs=tokenizer.texts_to_sequences(text)\n",
        "    seqs=tf.keras.preprocessing.sequence.pad_sequences(seqs,padding='post')\n",
        "    seqs=tf.convert_to_tensor(seqs)\n",
        "    return seqs\n",
        "def in_out_split(sens):\n",
        "    vie_in=[]\n",
        "    vie_out=[]\n",
        "    for sen in sens:\n",
        "        sen=sen.split()\n",
        "        vie_in_sen=' '.join(sen[:-1])\n",
        "        vie_out_sen=' '.join(sen[1:])\n",
        "        vie_in.append(vie_in_sen)\n",
        "        vie_out.append(vie_out_sen)\n",
        "    return tuple(vie_in),tuple(vie_out)\n",
        "def load_dataset(path,num_saple=None):\n",
        "    word,gloss,vie=create_dataset(path_to_file,num_saple)\n",
        "    #vie_in,vie_out=in_out_split(vie)\n",
        "    vie_tokenizer = tokenize(vie,VIE_VOCABULARY_SIZE)\n",
        "    eng_tokenizer = tokenize(word+gloss,ENG_VOCABULARY_SIZE)\n",
        "    word_tensor = padding(word,eng_tokenizer)\n",
        "    gloss_tensor = padding(gloss,eng_tokenizer)\n",
        "    vie_tensor = padding(vie,vie_tokenizer)\n",
        "    #vie_in_tensor = padding(vie_in,vie_tokenizer)\n",
        "    #vie_out_tensor = padding(vie_out,vie_tokenizer)\n",
        "    return word_tensor, gloss_tensor,vie_tensor,vie_tokenizer, eng_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUM_SAMPLE=150000\n",
        "BUFFER_SIZE = NUM_SAMPLE\n",
        "steps_per_epoch = NUM_SAMPLE//BATCH_SIZE\n",
        "ENG_VOCABULARY_SIZE=15000\n",
        "VIE_VOCABULARY_SIZE=20000\n",
        "EMBEDDING_DIM=300\n",
        "UNITS=1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "word_tensor, gloss_tensor,vie_tensor,vie_tokenizer, eng_tokenizer = load_dataset(path_to_file,NUM_SAMPLE)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((word_tensor,gloss_tensor, vie_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1BDsUovUby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word_embedding(filename=\"cc.en.300.vec\",DIR='/content/gdrive/My Drive/ColabNotebooks/word_embedding'):        \n",
        "    embeddings_index={}\n",
        "    f=open(os.path.join(DIR,filename),encoding=\"UTF-8\")\n",
        "    for line in f:\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    return embeddings_index\n",
        "def embedding_vocabulary(embedding_index,v):\n",
        "    words_not_found = []\n",
        "    vocab = len(v)+1\n",
        "    embedding_matrix = np.random.uniform(-0.25, 0.25, size=(vocab, 300)  )\n",
        "    for word,i in v.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            words_not_found.append(word)\n",
        "    # print('Number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "    print(\"\\tShape of embedding matrix: %s\" % str(embedding_matrix.shape))\n",
        "    print(\"\\tNo. of words not found in GloVe: \", len(words_not_found))\n",
        "    return embedding_matrix\n",
        "\n",
        "eng_embedding_index=load_word_embedding()\n",
        "eng_weights_matrix=embedding_vocabulary(eng_embedding_index,eng_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size+1, embedding_dim,embeddings_initializer=Constant(eng_weights_matrix))\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "  def call(self, x1,x2, hidden):\n",
        "    x1 = self.embedding(x1)\n",
        "    x2 = self.embedding(x2)\n",
        "    x2 = tf.math.reduce_mean(x2,1)\n",
        "    x2 = tf.expand_dims(x2,axis=1)\n",
        "    x = tf.math.reduce_mean([x1,x1,x2],0)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size+1, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x,initial_state = hidden)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(ENG_VOCABULARY_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
        "decoder = Decoder(VIE_VOCABULARY_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "checkpoint_dir = '/content/gdrive/My Drive/ColabNotebooks/my_text_generation/models/trying7.1'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp1,inp2, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp1,inp2, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([vie_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(targ[:, t], predictions)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {}
      },
      "source": [
        "EPOCHS =20\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp1,inp2, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp1,inp2, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "max_length=40\n",
        "def greedy(word,gloss):\n",
        "    gloss = preprocess_sentence(gloss)\n",
        "    gloss = remove_eng_stopword(gloss,stopword_set)\n",
        "    gloss = [eng_tokenizer.word_index[i] for i in gloss.split(' ')]\n",
        "    gloss = tf.keras.preprocessing.sequence.pad_sequences([gloss],padding='post')\n",
        "    gloss = tf.convert_to_tensor(gloss)\n",
        "    word = [eng_tokenizer.word_index[i] for i in word.split(' ')]\n",
        "    word = tf.keras.preprocessing.sequence.pad_sequences([word],padding='post')\n",
        "    word = tf.convert_to_tensor(word)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, UNITS))]\n",
        "    enc_out, enc_hidden = encoder(word,gloss, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([vie_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length):\n",
        "        predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        if vie_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result\n",
        "        result += vie_tokenizer.index_word[predicted_id] + ' '\n",
        "        if vie_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {}
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "'''print(checkpoint_dir)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}